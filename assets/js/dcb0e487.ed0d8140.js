"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9350],{9710:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var t=i(4848),o=i(8453);const s={title:"VAME step-by-step",sidebar_position:2},a=void 0,r={id:"getting_started/step_by_step",title:"VAME step-by-step",description:"The VAME workflow consists of four main steps, plus optional steps to analyse your data:",source:"@site/docs/getting_started/step_by_step.mdx",sourceDirName:"getting_started",slug:"/getting_started/step_by_step",permalink:"/VAME/docs/getting_started/step_by_step",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"VAME step-by-step",sidebar_position:2},sidebar:"docsSidebar",previous:{title:"Installation",permalink:"/VAME/docs/getting_started/installation"},next:{title:"Run Pipeline",permalink:"/VAME/docs/getting_started/run-pipeline"}},l={},d=[{value:"Running VAME, step-by-step",id:"running-vame-step-by-step",level:2},{value:"0. Download the necessary resources:",id:"0-download-the-necessary-resources",level:3},{value:"2. Running the demo main pipeline",id:"2-running-the-demo-main-pipeline",level:3},{value:"2.1a Setting the demo variables using CSV files",id:"21a-setting-the-demo-variables-using-csv-files",level:4},{value:"2.1b Setting the demo variables using NWB files",id:"21b-setting-the-demo-variables-using-nwb-files",level:4},{value:"2.2 Initializing the project",id:"22-initializing-the-project",level:4},{value:"2.3 Egocentric alignment",id:"23-egocentric-alignment",level:4},{value:"2.4 Creating the training dataset",id:"24-creating-the-training-dataset",level:4},{value:"2.5 Training the model",id:"25-training-the-model",level:4},{value:"2.6 Evaluate the model",id:"26-evaluate-the-model",level:4},{value:"2.7 Segmenting the behavior",id:"27-segmenting-the-behavior",level:4},{value:"3. Running Optional Steps of the Pipeline",id:"3-running-optional-steps-of-the-pipeline",level:3},{value:"3.1 Creating motif videos",id:"31-creating-motif-videos",level:4},{value:"3.2 Run community detection",id:"32-run-community-detection",level:4},{value:"3.3 Community Videos",id:"33-community-videos",level:4},{value:"3.4 UMAP Visualization",id:"34-umap-visualization",level:4},{value:"3.5 Generative Model (Reconstruction decoder)",id:"35-generative-model-reconstruction-decoder",level:4},{value:"3.6 Create output video",id:"36-create-output-video",level:4}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"The VAME workflow consists of four main steps, plus optional steps to analyse your data:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Initialize project: This is step is responsible by starting the project, getting your pose estimation data into the right format"}),"\n",(0,t.jsx)(n.li,{children:"Preprocess: This step is responsible for cleaning, filtering and aligning the pose estimation data"}),"\n",(0,t.jsxs)(n.li,{children:["Train neural network:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create a training dataset for the VAME deep learning model."}),"\n",(0,t.jsx)(n.li,{children:"Train a variational autoencoder which is parameterized with recurrent neural network to embed behavioural dynamics."}),"\n",(0,t.jsx)(n.li,{children:"Evaluate the performance of the trained model based on its reconstruction capabilities"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Segment behavior:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Segment pose estimation time series into behavioral motifs, using HMM or K-means."}),"\n",(0,t.jsx)(n.li,{children:"Group similar motifs into communities, using hierarchical clustering."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Analysis:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optional: Create motif videos to get insights about the fine grained poses."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create community videos to get more insights about behaviour on a hierarchical scale."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Visualization and projection of latent vectors onto a 2D plane via UMAP."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize the cluster centre for validation."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create a video of an egocentrically aligned animal + path through the community space."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Check out also the published VAME Workflow Guide, including more hands-on recommendations ",(0,t.jsx)(n.a,{href:"https://www.nature.com/articles/s42003-022-04080-7#Sec8",children:"HERE"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"running-vame-step-by-step",children:"Running VAME, step-by-step"}),"\n",(0,t.jsxs)(n.p,{children:["In this tutorial we will show you how to run the VAME workflow step-by-step using a simple example. The code below can also be found in the ",(0,t.jsx)(n.code,{children:"demo.py"})," script in our Github repository (",(0,t.jsx)(n.a,{href:"https://github.com/EthoML/VAME/blob/main/examples/demo.py",children:"link here"}),")."]}),"\n",(0,t.jsx)(n.h3,{id:"0-download-the-necessary-resources",children:"0. Download the necessary resources:"}),"\n",(0,t.jsx)(n.p,{children:"To run the demo you will need a video and a csv file with the pose estimation results. You can use the following files links:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"video-1.mp4"}),": Video file ",(0,t.jsx)(n.a,{href:"https://drive.google.com/file/d/1w6OW9cN_-S30B7rOANvSaR9c3O5KeF0c/view",children:"link"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"video-1.csv"}),": Pose estimation results ",(0,t.jsx)(n.a,{href:"https://github.com/EthoML/VAME/blob/master/examples/video-1.csv",children:"link"})]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-running-the-demo-main-pipeline",children:"2. Running the demo main pipeline"}),"\n",(0,t.jsx)(n.p,{children:"We will now show you how to run the main pipeline of the VAME workflow using  snnipets of code. We suggest you to run these snippets in a jupyter notebook."}),"\n",(0,t.jsx)(n.h4,{id:"21a-setting-the-demo-variables-using-csv-files",children:"2.1a Setting the demo variables using CSV files"}),"\n",(0,t.jsx)(n.p,{children:"To start the demo you must define 4 variables:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import vame\n\n\n# The directory where the project will be saved\nworking_directory = '.'\n\n# The name you want for the project\nproject = 'my-vame-project'\n\n# A list of paths to the videos file\nvideos = ['video-1.mp4']\n\n# A list of paths to the poses estimations files.\n# Important: The name (without the extension) of the video file and the pose estimation file must be the same. E.g. `video-1.mp4` and `video-1.csv`\nposes_estimations = ['video-1.csv']\n"})}),"\n",(0,t.jsx)(n.h4,{id:"21b-setting-the-demo-variables-using-nwb-files",children:"2.1b Setting the demo variables using NWB files"}),"\n",(0,t.jsxs)(n.p,{children:["Alternativaly you can use ",(0,t.jsx)(n.code,{children:".nwb"})," files as pose estimation files. In this case you must define 4 variables:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import vame\n\n\n# The directory where the project will be saved\nworking_directory = '.'\n\n# The name you want for the project\nproject = 'my-vame-project'\n\n# A list of paths to the videos file\nvideos = ['video-1.mp4']\n\n# A list of paths to the poses estimations files.\n# Important: The name (without the extension) of the video file and the pose estimation file must be the same. E.g. `video-1.mp4` and `video-1.nwb`\nposes_estimations = ['video-1.nwb']\n\n# A list of paths in the NWB file where the pose estimation data is stored.\npaths_to_pose_nwb_series_data = ['processing/behavior/data_interfaces/PoseEstimation/pose_estimation_series']\n"})}),"\n",(0,t.jsx)(n.h4,{id:"22-initializing-the-project",children:"2.2 Initializing the project"}),"\n",(0,t.jsx)(n.p,{children:"With the variables set, you can initialize the project by running the following code:"}),"\n",(0,t.jsx)(n.p,{children:"If you are using CSV files you can run the following code to initialize the project:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"config = vame.init_new_project(\n    project=project,\n    videos=videos,\n    poses_estimations=poses_estimations,\n    working_directory=working_directory,\n    videotype='.mp4'\n)\n"})}),"\n",(0,t.jsx)(n.p,{children:"If you are using NWB files you can run the following code to initialize the project:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"config = vame.init_new_project(\n    project=project,\n    videos=videos,\n    poses_estimations=poses_estimations,\n    working_directory=working_directory,\n    videotype='.mp4',\n    paths_to_pose_nwb_series_data=paths_to_pose_nwb_series_data\n)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This command will create a project folder in the defined working directory with the name you set in the ",(0,t.jsx)(n.code,{children:"project"})," variable and a date suffix, e.g: ",(0,t.jsx)(n.code,{children:"my-vame-project-May-9-2024"}),".\nIn this folder you can find a config file called ",(0,t.jsx)(n.code,{children:"config.yaml"})," where you can set the parameters for the VAME algorithm.\nThe videos and poses estimations files will be copied to the project videos folder. It is really important to define in the ",(0,t.jsx)(n.code,{children:"config.yaml"})," file if your data is egocentrically aligned or not before running the rest of the workflow."]}),"\n",(0,t.jsx)(n.h4,{id:"23-egocentric-alignment",children:"2.3 Egocentric alignment"}),"\n",(0,t.jsx)(n.p,{children:"If your data is not egocentrically aligned, you can align it by running the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.egocentric_alignment(config, pose_ref_index=[0, 5])\n"})}),"\n",(0,t.jsx)(n.p,{children:"But if your experiment is by design egocentrical (e.g. head-fixed experiment on treadmill etc) you can use the following to convert your .csv to a .npy array, ready to train vame on it."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.csv_to_numpy(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"24-creating-the-training-dataset",children:"2.4 Creating the training dataset"}),"\n",(0,t.jsx)(n.p,{children:"To create the training dataset you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.create_trainset(config, pose_ref_index=[0,5])\n"})}),"\n",(0,t.jsx)(n.h4,{id:"25-training-the-model",children:"2.5 Training the model"}),"\n",(0,t.jsx)(n.p,{children:"Training the vame model might take a while depending on the size of your dataset and your machine settings. To train the model you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.train_model(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"26-evaluate-the-model",children:"2.6 Evaluate the model"}),"\n",(0,t.jsx)(n.p,{children:"THe model evaluation produces two plots, one showing the loss of the model during training and the other showing the reconstruction and future prediction of input sequence."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.evaluate_model(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"27-segmenting-the-behavior",children:"2.7 Segmenting the behavior"}),"\n",(0,t.jsx)(n.p,{children:"To perform pose segmentation you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.pose_segmentation(config)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-running-optional-steps-of-the-pipeline",children:"3. Running Optional Steps of the Pipeline"}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"The following steps are optional and can be run if you want to create motif VideoColorSpace, communities/hierarchies of behavior and community VideoColorSpace."})}),"\n",(0,t.jsx)(n.h4,{id:"31-creating-motif-videos",children:"3.1 Creating motif videos"}),"\n",(0,t.jsx)(n.p,{children:"To create motif videos and get insights about the fine grained poses you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.motif_videos(config, videoType='.mp4')\n"})}),"\n",(0,t.jsx)(n.h4,{id:"32-run-community-detection",children:"3.2 Run community detection"}),"\n",(0,t.jsx)(n.p,{children:"To create behavioral hierarchies and communities detection run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.community(config, parametrization='hmm', cut_tree=2, cohort=False)\n"})}),"\n",(0,t.jsx)(n.p,{children:"It will produce a tree plot of the behavioural hierarchies using hmm motifs."}),"\n",(0,t.jsx)(n.h4,{id:"33-community-videos",children:"3.3 Community Videos"}),"\n",(0,t.jsx)(n.p,{children:"Create community videos to get insights about behavior on a hierarchical scale."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.community_videos(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"34-umap-visualization",children:"3.4 UMAP Visualization"}),"\n",(0,t.jsx)(n.p,{children:"Down projection of latent vectors and visualization via UMAP."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'fig = vame.visualization(config, label=None) #options: label: None, "motif", "community"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"35-generative-model-reconstruction-decoder",children:"3.5 Generative Model (Reconstruction decoder)"}),"\n",(0,t.jsx)(n.p,{children:"Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize\nthe cluster center for validation."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'vame.generative_model(config, mode="centers") #options: mode: "sampling", "reconstruction", "centers", "motifs"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"36-create-output-video",children:"3.6 Create output video"}),"\n",(0,t.jsx)(n.p,{children:"Create a video of an egocentrically aligned mouse + path through\nthe community space (similar to our gif on github) to learn more about your representation\nand have something cool to show around."}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"This function is currently very slow."})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.gif(config, pose_ref_index=[0,5], subtract_background=True, start=None,\n         length=500, max_lag=30, label='community', file_format='.mp4', crop_size=(300,300))\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Once the frames are saved you can create a video or gif via e.g. ImageJ or other tools"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);