"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3976],{1512:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>d});var a=n(4848),o=n(8453);const i={title:"Introduction",sidebar_position:1},s=void 0,r={id:"intro",title:"Introduction",description:"\ud83c\udf1f Welcome to EthoML/VAME (Variational Animal Motion Encoding), an open-source machine learning tool for behavioral segmentation and analyses.",source:"@site/docs/intro.md",sourceDirName:".",slug:"/intro",permalink:"/docs/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Introduction",sidebar_position:1},sidebar:"docsSidebar",next:{title:"Getting Started",permalink:"/docs/category/getting-started"}},c={},d=[{value:"VAME in a Nutshell",id:"vame-in-a-nutshell",level:2},{value:"Authors and Code Contributors",id:"authors-and-code-contributors",level:2}];function l(e){const t={a:"a",h2:"h2",img:"img",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"\ud83c\udf1f Welcome to EthoML/VAME (Variational Animal Motion Encoding), an open-source machine learning tool for behavioral segmentation and analyses."}),"\n",(0,a.jsx)(t.p,{children:"We are a group of behavioral enthusiasts, comprising the original VAME developers Kevin Luxem and Pavol Bauer, behavioral neuroscientists Stephanie R. Miller and Jorge J. Palop, and computer scientists and statisticians Alex Pico, Reuben Thomas, and Katie Ly). Our aim is to provide scalable, unbiased and sensitive approaches for assessing mouse behavior using computer vision and machine learning approaches."}),"\n",(0,a.jsx)(t.p,{children:"We are focused on the expanding the analytical capabilities of VAME segmentation by providing curated scripts for VAME implementation and tools for data processing, visualization, and statistical analyses."}),"\n",(0,a.jsx)(t.h2,{id:"vame-in-a-nutshell",children:"VAME in a Nutshell"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"VAME",src:n(6045).A+"",width:"538",height:"255"})}),"\n",(0,a.jsxs)(t.p,{children:["VAME is a framework to cluster behavioral signals obtained from pose-estimation tools. It is a ",(0,a.jsx)(t.a,{href:"https://pytorch.org/",children:"PyTorch"}),"-based deep learning framework which leverages the power of recurrent neural networks (RNN) to model sequential data. In order to learn the underlying complex data distribution, we use the RNN in a variational autoencoder setting to extract the latent state of the animal in every step of the input time series.\nThe workflow of VAME consists of 5 steps and we explain them in detail ",(0,a.jsx)(t.a,{href:"https://github.com/LINCellularNeuroscience/VAME/wiki/1.-VAME-Workflow",children:"here"})]}),"\n",(0,a.jsx)(t.h2,{id:"authors-and-code-contributors",children:"Authors and Code Contributors"}),"\n",(0,a.jsxs)(t.p,{children:["VAME was developed by ",(0,a.jsx)(t.strong,{children:"Kevin Luxem"})," and ",(0,a.jsx)(t.strong,{children:"Pavol Bauer"})," (Luxem et. al., 2022). The original VAME repository was deprecated, forked, and is now being maintained here at ",(0,a.jsx)(t.a,{href:"https://github.com/EthoML/VAME",children:"https://github.com/EthoML/VAME"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["The development of VAME is heavily inspired by ",(0,a.jsx)(t.a,{href:"https://github.com/DeepLabCut/DeepLabCut/",children:"DeepLabCut"}),". As such, the VAME project management codebase has been adapted from the DeepLabCut codebase. The DeepLabCut 2.0 toolbox is \xa9 A. & M.W. Mathis Labs ",(0,a.jsx)(t.a,{href:"http:%5Cdeeplabcut.org",children:"deeplabcut.org"}),", released under LGPL v3.0. The implementation of the VRAE model is partially adapted from the ",(0,a.jsx)(t.a,{href:"https://github.com/tejaslodaya/timeseries-clustering-vae",children:"Timeseries clustering"})," repository developed by ",(0,a.jsx)(t.a,{href:"https://tejaslodaya.com",children:"Tejas Lodaya"}),"."]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},6045:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/behavior_structure_crop-454f14cb814a0843e2459d7ba99bce63.gif"},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>r});var a=n(6540);const o={},i=a.createContext(o);function s(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);