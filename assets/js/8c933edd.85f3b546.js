"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1864],{5559:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>p,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var t=i(4848),s=i(8453);const o={id:"pipeline",title:"VAME Pipeline",sidebar_position:3,slug:"/getting_started/pipeline"},r=void 0,a={id:"getting_started/pipeline",title:"VAME Pipeline",description:"Open In Colab",source:"@site/docs/getting_started/pipeline.mdx",sourceDirName:"getting_started",slug:"/getting_started/pipeline",permalink:"/VAME/docs/getting_started/pipeline",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{id:"pipeline",title:"VAME Pipeline",sidebar_position:3,slug:"/getting_started/pipeline"},sidebar:"docsSidebar",previous:{title:"VAME step-by-step",permalink:"/VAME/docs/getting_started/step_by_step"},next:{title:"Project Configuration",permalink:"/VAME/docs/project-config"}},p={},l=[{value:"Input data",id:"input-data",level:2},{value:"Instantiate the VAME pipeline",id:"instantiate-the-vame-pipeline",level:2},{value:"Run the pipeline",id:"run-the-pipeline",level:2},{value:"Visualize the results",id:"visualize-the-results",level:2},{value:"Produce the pipeline report",id:"produce-the-pipeline-report",level:2},{value:"Resuming the pipeline",id:"resuming-the-pipeline",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://colab.research.google.com/github/EthoML/VAME/blob/main/examples/pipeline.ipynb",children:(0,t.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})})}),"\n",(0,t.jsx)(n.p,{children:"If you haven't yet, please install VAME:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install vame-py\n"})}),"\n",(0,t.jsx)(n.p,{children:"Ther most convenient way to use VAME is through the VAME pipeline. The pipeline class automates the processes of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"loading the data"}),"\n",(0,t.jsx)(n.li,{children:"preprocessing the data"}),"\n",(0,t.jsx)(n.li,{children:"creating and training the VAME model"}),"\n",(0,t.jsx)(n.li,{children:"validating the VAME model"}),"\n",(0,t.jsx)(n.li,{children:"segmenting the behavior into motifs"}),"\n",(0,t.jsx)(n.li,{children:"clustering the motifs into communities"}),"\n",(0,t.jsx)(n.li,{children:"visualizing the results"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Let's start by importing the necessary libraries:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import pprint\nfrom vame.pipeline import VAMEPipeline\nfrom vame.util.sample_data import download_sample_data\n"})}),"\n",(0,t.jsx)(n.h2,{id:"input-data",children:"Input data"}),"\n",(0,t.jsx)(n.p,{children:"To quickly try VAME, you can download sample data and use it as input. If you want to work with your own data, all you need to do is to provide the paths to the pose estimation files as lists of strings. You can also optionally provide the paths to the corresponding video files."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# You can run VAME with data from different sources:\n# "DeepLabCut", "SLEAP" or "LightningPose"\nsource_software = "DeepLabCut"\n\n# Download sample data\nps = download_sample_data(source_software)\nvideos = [ps["video"]]\nposes_estimations = [ps["poses"]]\n\npprint.pp(videos)\npprint.pp(poses_estimations)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"instantiate-the-vame-pipeline",children:"Instantiate the VAME pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Now it's time to instantiate the VAME pipeline. Select your working directory, name of your project and extra configuration arguments. The extra configuration arguments are optional and can be used to customize the VAME pipeline."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Set up your working directory and project name\nworking_directory = \'.\'\nproject_name = \'pipeline_example\'\n\n# Customize the configuration for the project\nconfig_kwargs = {\n    "n_clusters": 15,\n    "pose_confidence": 0.9,\n    "max_epochs": 100,\n}\n\n# Instantiate the pipeline\n# this will create a VAME project and prepare the data\npipeline = VAMEPipeline(\n    working_directory=working_directory,\n    project_name=project_name,\n    videos=videos,\n    poses_estimations=poses_estimations,\n    source_software=source_software,\n    config_kwargs=config_kwargs,\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"Before running the pipeline, you can check the input datasets:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"ds = pipeline.get_raw_datasets()\nds\n"})}),"\n",(0,t.jsx)(n.h2,{id:"run-the-pipeline",children:"Run the pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Now you can run the pipeline. At this point, you should pass the names of the pose estimation keypoints to be used for egocentric alignment."}),"\n",(0,t.jsx)(n.p,{children:"Note: The pipeline will take some time to run, depending on the size of the dataset, number of epochs, and if you are using a GPU or not."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'preprocessing_kwargs = {\n    "centered_reference_keypoint": "snout",\n    "orientation_reference_keypoint": "tailbase",\n}\npipeline.run_pipeline(preprocessing_kwargs=preprocessing_kwargs)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"visualize-the-results",children:"Visualize the results"}),"\n",(0,t.jsx)(n.p,{children:"After running the pipeline, you can visualize the results:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pipeline.visualize_preprocessing(\n    show_figure=True,\n    save_to_file=False,\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pipeline.visualize_model_losses(\n    show_figure=True,\n    save_to_file=False,\n)\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline.visualize_motif_tree(segmentation_algorithm="hmm")\n'})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pipeline.visualize_umap(\n    label="community",\n    segmentation_algorithm="hmm",\n    show_figure=True,\n)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"produce-the-pipeline-report",children:"Produce the pipeline report"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pipeline.report()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"resuming-the-pipeline",children:"Resuming the pipeline"}),"\n",(0,t.jsx)(n.p,{children:"If for some reason you need to stop the pipeline, you can resume it later from any step\nExample: resuming from community clustering step"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"pipeline.run_pipeline(from_step=5)\n"})})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);