"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9350],{9710:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});var t=i(4848),o=i(8453);const a={title:"VAME step-by-step",sidebar_position:2},r=void 0,s={id:"getting_started/step_by_step",title:"VAME step-by-step",description:"The VAME workflow consists of four main steps, plus optional steps to analyse your data:",source:"@site/docs/getting_started/step_by_step.mdx",sourceDirName:"getting_started",slug:"/getting_started/step_by_step",permalink:"/VAME/docs/getting_started/step_by_step",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{title:"VAME step-by-step",sidebar_position:2},sidebar:"docsSidebar",previous:{title:"Installation",permalink:"/VAME/docs/getting_started/installation"},next:{title:"Run Pipeline",permalink:"/VAME/docs/getting_started/pipeline"}},l={},d=[{value:"0. [Optional] Download input data",id:"0-optional-download-input-data",level:2},{value:"1. Initialize the project",id:"1-initialize-the-project",level:2},{value:"2.3 Egocentric alignment",id:"23-egocentric-alignment",level:4},{value:"2.4 Creating the training dataset",id:"24-creating-the-training-dataset",level:4},{value:"2.5 Training the model",id:"25-training-the-model",level:4},{value:"2.6 Evaluate the model",id:"26-evaluate-the-model",level:4},{value:"2.7 Segmenting the behavior",id:"27-segmenting-the-behavior",level:4},{value:"3. Running Optional Steps of the Pipeline",id:"3-running-optional-steps-of-the-pipeline",level:3},{value:"3.1 Creating motif videos",id:"31-creating-motif-videos",level:4},{value:"3.2 Run community detection",id:"32-run-community-detection",level:4},{value:"3.3 Community Videos",id:"33-community-videos",level:4},{value:"3.4 UMAP Visualization",id:"34-umap-visualization",level:4},{value:"3.5 Generative Model (Reconstruction decoder)",id:"35-generative-model-reconstruction-decoder",level:4},{value:"3.6 Create output video",id:"36-create-output-video",level:4}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"The VAME workflow consists of four main steps, plus optional steps to analyse your data:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Initialize project: This is step is responsible by starting the project, getting your pose estimation data into the right format"}),"\n",(0,t.jsx)(n.li,{children:"Preprocess: This step is responsible for cleaning, filtering and aligning the pose estimation data"}),"\n",(0,t.jsxs)(n.li,{children:["Train neural network:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create a training dataset for the VAME deep learning model."}),"\n",(0,t.jsx)(n.li,{children:"Train a variational autoencoder which is parameterized with recurrent neural network to embed behavioural dynamics."}),"\n",(0,t.jsx)(n.li,{children:"Evaluate the performance of the trained model based on its reconstruction capabilities"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Segment behavior:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Segment pose estimation time series into behavioral motifs, using HMM or K-means."}),"\n",(0,t.jsx)(n.li,{children:"Group similar motifs into communities, using hierarchical clustering."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["Analysis:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optional: Create motif videos to get insights about the fine grained poses."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create community videos to get more insights about behaviour on a hierarchical scale."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Visualization and projection of latent vectors onto a 2D plane via UMAP."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize the cluster centre for validation."}),"\n",(0,t.jsx)(n.li,{children:"Optional: Create a video of an egocentrically aligned animal + path through the community space."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["In this tutorial we will show you how to run the VAME workflow using a simple example. The code below can also be found in the ",(0,t.jsx)(n.code,{children:"demo.py"})," script in our Github repository (",(0,t.jsx)(n.a,{href:"https://github.com/EthoML/VAME/blob/main/examples/demo.py",children:"link here"}),")."]}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["Check out also the published VAME Workflow Guide, including more hands-on recommendations ",(0,t.jsx)(n.a,{href:"https://www.nature.com/articles/s42003-022-04080-7#Sec8",children:"HERE"}),"."]})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsxs)(n.p,{children:["You can run an entire VAME workflow with just a few lines, using the ",(0,t.jsx)(n.a,{href:"/docs/getting_started/pipeline",children:"Pipeline method"}),"."]})}),"\n",(0,t.jsx)(n.h2,{id:"0-optional-download-input-data",children:"0. [Optional] Download input data"}),"\n",(0,t.jsx)(n.p,{children:"To run VAME you will need a video and a pose estimation file. If you don't have your own data, you download sample data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from vame.util.sample_data import download_sample_data\n\nsource_software = "DeepLabCut"              # "DeepLabCut", "SLEAP" or "LightningPose"\nps = download_sample_data(source_software)  # Data will be downloaded to ~/.movement/data/\nvideos = [ps["video"]]                      # List of paths to the video files\nposes_estimations = [ps["poses"]]           # List of paths to the pose estimation files\n'})}),"\n",(0,t.jsx)(n.h2,{id:"1-initialize-the-project",children:"1. Initialize the project"}),"\n",(0,t.jsx)(n.p,{children:"VAME organizes around projects. To start a new project, you need to define a few things:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import vame\n\nworking_directory = \'.\'      # The directory where the project will be saved\nproject = \'my-vame-project\'  # The name of the project\n\n# [Optional] Customized configuration for the project\nconfig_kwargs = {\n    "n_clusters": 15,\n    "pose_confidence": 0.9,\n    "max_epochs": 100,\n}\n\nconfig_path, config = vame.init_new_project(\n    project_name=project_name,\n    videos=videos,\n    poses_estimations=poses_estimations,\n    source_software=source_software,\n    working_directory=working_directory,\n    config_kwargs=config_kwargs,\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["This command will create a project folder in the defined working directory with the project name you defined.\nIn this folder you can find a config file called ",(0,t.jsx)(n.code,{children:"config.yaml"})," which holds the main parameters for the VAME workflow.\nThe videos and pose estimation files will be linked or copied to the project folder."]}),"\n",(0,t.jsx)(n.h4,{id:"23-egocentric-alignment",children:"2.3 Egocentric alignment"}),"\n",(0,t.jsx)(n.p,{children:"If your data is not egocentrically aligned, you can align it by running the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.egocentric_alignment(config, pose_ref_index=[0, 5])\n"})}),"\n",(0,t.jsx)(n.p,{children:"But if your experiment is by design egocentrical (e.g. head-fixed experiment on treadmill etc) you can use the following to convert your .csv to a .npy array, ready to train vame on it."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.csv_to_numpy(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"24-creating-the-training-dataset",children:"2.4 Creating the training dataset"}),"\n",(0,t.jsx)(n.p,{children:"To create the training dataset you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.create_trainset(config, pose_ref_index=[0,5])\n"})}),"\n",(0,t.jsx)(n.h4,{id:"25-training-the-model",children:"2.5 Training the model"}),"\n",(0,t.jsx)(n.p,{children:"Training the vame model might take a while depending on the size of your dataset and your machine settings. To train the model you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.train_model(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"26-evaluate-the-model",children:"2.6 Evaluate the model"}),"\n",(0,t.jsx)(n.p,{children:"THe model evaluation produces two plots, one showing the loss of the model during training and the other showing the reconstruction and future prediction of input sequence."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.evaluate_model(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"27-segmenting-the-behavior",children:"2.7 Segmenting the behavior"}),"\n",(0,t.jsx)(n.p,{children:"To perform pose segmentation you can run the following code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.pose_segmentation(config)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-running-optional-steps-of-the-pipeline",children:"3. Running Optional Steps of the Pipeline"}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"The following steps are optional and can be run if you want to create motif VideoColorSpace, communities/hierarchies of behavior and community VideoColorSpace."})}),"\n",(0,t.jsx)(n.h4,{id:"31-creating-motif-videos",children:"3.1 Creating motif videos"}),"\n",(0,t.jsx)(n.p,{children:"To create motif videos and get insights about the fine grained poses you can run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.motif_videos(config, videoType='.mp4')\n"})}),"\n",(0,t.jsx)(n.h4,{id:"32-run-community-detection",children:"3.2 Run community detection"}),"\n",(0,t.jsx)(n.p,{children:"To create behavioral hierarchies and communities detection run:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.community(config, parametrization='hmm', cut_tree=2, cohort=False)\n"})}),"\n",(0,t.jsx)(n.p,{children:"It will produce a tree plot of the behavioural hierarchies using hmm motifs."}),"\n",(0,t.jsx)(n.h4,{id:"33-community-videos",children:"3.3 Community Videos"}),"\n",(0,t.jsx)(n.p,{children:"Create community videos to get insights about behavior on a hierarchical scale."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.community_videos(config)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"34-umap-visualization",children:"3.4 UMAP Visualization"}),"\n",(0,t.jsx)(n.p,{children:"Down projection of latent vectors and visualization via UMAP."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'fig = vame.visualization(config, label=None) #options: label: None, "motif", "community"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"35-generative-model-reconstruction-decoder",children:"3.5 Generative Model (Reconstruction decoder)"}),"\n",(0,t.jsx)(n.p,{children:"Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize\nthe cluster center for validation."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'vame.generative_model(config, mode="centers") #options: mode: "sampling", "reconstruction", "centers", "motifs"\n'})}),"\n",(0,t.jsx)(n.h4,{id:"36-create-output-video",children:"3.6 Create output video"}),"\n",(0,t.jsx)(n.p,{children:"Create a video of an egocentrically aligned mouse + path through\nthe community space (similar to our gif on github) to learn more about your representation\nand have something cool to show around."}),"\n",(0,t.jsx)(n.admonition,{type:"warning",children:(0,t.jsx)(n.p,{children:"This function is currently very slow."})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"vame.gif(config, pose_ref_index=[0,5], subtract_background=True, start=None,\n         length=500, max_lag=30, label='community', file_format='.mp4', crop_size=(300,300))\n"})}),"\n",(0,t.jsx)(n.admonition,{type:"tip",children:(0,t.jsx)(n.p,{children:"Once the frames are saved you can create a video or gif via e.g. ImageJ or other tools"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);