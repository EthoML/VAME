"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4841],{2383:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>d,default:()=>h,frontMatter:()=>s,metadata:()=>c,toc:()=>o});var r=i(4848),l=i(8453);const s={sidebar_label:"align_egocentrical",title:"vame.util.align_egocentrical"},d=void 0,c={id:"reference/vame/util/align_egocentrical",title:"vame.util.align_egocentrical",description:"Variational Animal Motion Embedding 0.1 Toolbox",source:"@site/docs/reference/vame/util/align_egocentrical.md",sourceDirName:"reference/vame/util",slug:"/reference/vame/util/align_egocentrical",permalink:"/VAME/docs/reference/vame/util/align_egocentrical",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{sidebar_label:"align_egocentrical",title:"vame.util.align_egocentrical"},sidebar:"docsSidebar",previous:{title:"rnn_vae",permalink:"/VAME/docs/reference/vame/model/rnn_vae"},next:{title:"auxiliary",permalink:"/VAME/docs/reference/vame/util/auxiliary"}},t={},o=[{value:"crop_and_flip",id:"crop_and_flip",level:4},{value:"nan_helper",id:"nan_helper",level:4},{value:"interpol",id:"interpol",level:4},{value:"background",id:"background",level:4},{value:"align_mouse",id:"align_mouse",level:4},{value:"play_aligned_video",id:"play_aligned_video",level:4},{value:"alignment",id:"alignment",level:4},{value:"egocentric_alignment",id:"egocentric_alignment",level:4}];function a(e){const n={a:"a",code:"code",em:"em",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Variational Animal Motion Embedding 0.1 Toolbox\n\xa9 K. Luxem & J. K\xfcrsch & P. Bauer, Department of Cellular Neuroscience\nLeibniz Institute for Neurobiology, Magdeburg, Germany"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://github.com/LINCellularNeuroscience/VAME",children:"https://github.com/LINCellularNeuroscience/VAME"}),"\nLicensed under GNU General Public License v3.0"]}),"\n",(0,r.jsx)(n.h4,{id:"crop_and_flip",children:"crop_and_flip"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def crop_and_flip(\n        rect: Tuple, src: np.ndarray, points: List[np.ndarray],\n        ref_index: Tuple[int, int]) -> Tuple[np.ndarray, List[np.ndarray]]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Crop and flip the image based on the given rectangle and points."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"rect"})," ",(0,r.jsx)(n.em,{children:"Tuple"})," - Rectangle coordinates (center, size, theta)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"src"})," ",(0,r.jsx)(n.em,{children:"np.ndarray"})," - Source image."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"points"})," ",(0,r.jsx)(n.em,{children:"List[np.ndarray]"})," - List of points."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"ref_index"})," ",(0,r.jsx)(n.em,{children:"Tuple[int, int]"})," - Reference indices for alignment."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,r.jsx)(n.p,{children:"Tuple[np.ndarray, List[np.ndarray]]: Cropped and flipped image, and shifted points."}),"\n",(0,r.jsx)(n.h4,{id:"nan_helper",children:"nan_helper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def nan_helper(y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Helper function to identify NaN values in an array."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"y"})," ",(0,r.jsx)(n.em,{children:"np.ndarray"})," - Input array."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,r.jsx)(n.p,{children:"Tuple[np.ndarray, np.ndarray]: Boolean mask for NaN values and function to interpolate them."}),"\n",(0,r.jsx)(n.h4,{id:"interpol",children:"interpol"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def interpol(arr: np.ndarray) -> np.ndarray\n"})}),"\n",(0,r.jsx)(n.p,{children:"Interpolates NaN values in the given array."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"arr"})," ",(0,r.jsx)(n.em,{children:"np.ndarray"})," - Input array."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"np.ndarray"})," - Array with interpolated NaN values."]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"background",children:"background"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def background(path_to_file: str,\n               filename: str,\n               video_format: str = '.mp4',\n               num_frames: int = 1000) -> np.ndarray\n"})}),"\n",(0,r.jsx)(n.p,{children:"Compute the background image from a fixed camera."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"path_to_file"})," ",(0,r.jsx)(n.em,{children:"str"})," - Path to the file directory."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"filename"})," ",(0,r.jsx)(n.em,{children:"str"})," - Name of the video file without the format."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"video_format"})," ",(0,r.jsx)(n.em,{children:"str, optional"})," - Format of the video file. Defaults to '.mp4'."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"num_frames"})," ",(0,r.jsx)(n.em,{children:"int, optional"})," - Number of frames to use for background computation. Defaults to 1000."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"np.ndarray"})," - Background image."]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"align_mouse",children:"align_mouse"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def align_mouse(\n    path_to_file: str,\n    filename: str,\n    video_format: str,\n    crop_size: Tuple[int, int],\n    pose_list: List[np.ndarray],\n    pose_ref_index: Tuple[int, int],\n    confidence: float,\n    pose_flip_ref: Tuple[int, int],\n    bg: np.ndarray,\n    frame_count: int,\n    use_video: bool = True\n) -> Tuple[List[np.ndarray], List[List[np.ndarray]], np.ndarray]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Align the mouse in the video frames."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"path_to_file"})," ",(0,r.jsx)(n.em,{children:"str"})," - Path to the file directory."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"filename"})," ",(0,r.jsx)(n.em,{children:"str"})," - Name of the video file without the format."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"video_format"})," ",(0,r.jsx)(n.em,{children:"str"})," - Format of the video file."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"crop_size"})," ",(0,r.jsx)(n.em,{children:"Tuple[int, int]"})," - Size to crop the video frames."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"pose_list"})," ",(0,r.jsx)(n.em,{children:"List[np.ndarray]"})," - List of pose coordinates."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"pose_ref_index"})," ",(0,r.jsx)(n.em,{children:"Tuple[int, int]"})," - Pose reference indices."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"confidence"})," ",(0,r.jsx)(n.em,{children:"float"})," - Pose confidence threshold."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"pose_flip_ref"})," ",(0,r.jsx)(n.em,{children:"Tuple[int, int]"})," - Reference indices for flipping."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"bg"})," ",(0,r.jsx)(n.em,{children:"np.ndarray"})," - Background image."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"frame_count"})," ",(0,r.jsx)(n.em,{children:"int"})," - Number of frames to align."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"filename"}),"0 ",(0,r.jsx)(n.em,{children:"bool, optional"})," - bool if video should be cropped or DLC points only. Defaults to True."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,r.jsx)(n.p,{children:"Tuple[List[np.ndarray], List[List[np.ndarray]], np.ndarray]: List of aligned images, list of aligned DLC points, and time series data."}),"\n",(0,r.jsx)(n.h4,{id:"play_aligned_video",children:"play_aligned_video"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def play_aligned_video(a: List[np.ndarray], n: List[List[np.ndarray]],\n                       frame_count: int) -> None\n"})}),"\n",(0,r.jsx)(n.p,{children:"Play the aligned video."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"a"})," ",(0,r.jsx)(n.em,{children:"List[np.ndarray]"})," - List of aligned images."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"n"})," ",(0,r.jsx)(n.em,{children:"List[List[np.ndarray]]"})," - List of aligned DLC points."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"frame_count"})," ",(0,r.jsx)(n.em,{children:"int"})," - Number of frames in the video."]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"alignment",children:"alignment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def alignment(\n        path_to_file: str,\n        filename: str,\n        pose_ref_index: List[int],\n        video_format: str,\n        crop_size: Tuple[int, int],\n        confidence: float,\n        use_video: bool = False,\n        check_video: bool = False) -> Tuple[np.ndarray, List[np.ndarray]]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Perform alignment of egocentric data."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"path_to_file"})," ",(0,r.jsx)(n.em,{children:"str"})," - Path to the file directory."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"filename"})," ",(0,r.jsx)(n.em,{children:"str"})," - Name of the video file without the format."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"pose_ref_index"})," ",(0,r.jsx)(n.em,{children:"List[int]"})," - Pose reference indices."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"video_format"})," ",(0,r.jsx)(n.em,{children:"str"})," - Format of the video file."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"crop_size"})," ",(0,r.jsx)(n.em,{children:"Tuple[int, int]"})," - Size to crop the video frames."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"confidence"})," ",(0,r.jsx)(n.em,{children:"float"})," - Pose confidence threshold."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"use_video"})," ",(0,r.jsx)(n.em,{children:"bool, optional"})," - Whether to use video for alignment. Defaults to False."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"check_video"})," ",(0,r.jsx)(n.em,{children:"bool, optional"})," - Whether to check the aligned video. Defaults to False."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,r.jsx)(n.p,{children:"Tuple[np.ndarray, List[np.ndarray]]: Aligned time series data and list of aligned frames."}),"\n",(0,r.jsx)(n.h4,{id:"egocentric_alignment",children:"egocentric_alignment"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"def egocentric_alignment(config: str,\n                         pose_ref_index: list = [5, 6],\n                         crop_size: tuple = (300, 300),\n                         use_video: bool = False,\n                         video_format: str = '.mp4',\n                         check_video: bool = False) -> None\n"})}),"\n",(0,r.jsx)(n.p,{children:"Aligns egocentric data for VAME training"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"config"})," ",(0,r.jsx)(n.em,{children:"str"})," - Path for the project config file."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"pose_ref_index"})," ",(0,r.jsx)(n.em,{children:"list, optional"})," - Pose reference index to be used to align. Defaults to [5,6]."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"crop_size"})," ",(0,r.jsx)(n.em,{children:"tuple, optional"})," - Size to crop the video. Defaults to (300,300)."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"use_video"})," ",(0,r.jsx)(n.em,{children:"bool, optional"})," - Weather to use video to do the post alignment. Defaults to False. # TODO check what to put in this docstring"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"video_format"})," ",(0,r.jsx)(n.em,{children:"str, optional"})," - Video format, can be .mp4 or .avi. Defaults to '.mp4'."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"check_video"})," ",(0,r.jsx)(n.em,{children:"bool, optional"})," - Weather to check the video. Defaults to False."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Raises"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"ValueError"})," - If the config.yaml indicates that the data is not egocentric."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(a,{...e})}):a(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>d,x:()=>c});var r=i(6540);const l={},s=r.createContext(l);function d(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:d(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);