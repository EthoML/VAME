{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfb5249-3464-4172-a212-cd84c4cd0723",
   "metadata": {},
   "source": [
    "# VAME tutorial\n",
    "\n",
    "In this tutorial we'll go through an entire VAME pipeline, step-by-step, including:\n",
    "- loading the data\n",
    "- preprocessing the data\n",
    "- training the VAME model\n",
    "- evaluating the VAME model\n",
    "- segmenting the behavior into motifs\n",
    "- clustering the motifs into communities\n",
    "- visualizing the results\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c4951-4ff1-4604-be12-0a7851aef6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513720cd-5df8-4f2d-9ab8-248594e0dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vame\n",
    "from vame.util.sample_data import download_sample_data\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475247b2-029d-4f25-8080-b90bfbb579a2",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "To quickly try VAME, you can download sample data and use it as input. If you want to work with your own data, all you need to do is to provide the paths to the pose estimation files as lists of strings. You can also optionally provide the paths to the corresponding video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329eb6ae-e676-4f30-a54a-90e8b179ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run VAME with data from different sources:\n",
    "# \"DeepLabCut\", \"SLEAP\" or \"LightningPose\"\n",
    "source_software = \"DeepLabCut\"\n",
    "\n",
    "# Download sample data\n",
    "ps = download_sample_data(source_software)\n",
    "videos = [ps[\"video\"]]\n",
    "poses_estimations = [ps[\"poses\"]]\n",
    "\n",
    "print(videos)\n",
    "print(poses_estimations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848be459-37b1-4607-81a3-686cc0df9c0d",
   "metadata": {},
   "source": [
    "## Step 1: Initialize your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845cce0-5711-474c-b249-f0f539b9329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file, config_data = vame.init_new_project(\n",
    "    project_name=\"my_vame_project\",\n",
    "    poses_estimations=poses_estimations,\n",
    "    source_software=\"DeepLabCut\",\n",
    "    videos=videos,\n",
    "    video_type='.mp4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cae612-d0a9-47e4-a006-bd57a2c31c39",
   "metadata": {},
   "source": [
    "Let's take a look at the project's configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab25e0-6e14-47f9-93b6-2d9100df56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d6414-2d34-462e-9d2d-0aefe304a4ba",
   "metadata": {},
   "source": [
    "Now let's take a look at the input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299160d-b9cf-48d7-bdc7-f8576a1977a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = Path(config_data[\"project_path\"]) / \"data\" / \"raw\" / f\"{config_data['session_names'][0]}.nc\"\n",
    "vame.io.load_poses.load_vame_dataset(ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562863d9-62f0-4827-a61f-63aa980912fd",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the raw pose estimation data\n",
    "The preprocessing step includes:\n",
    "- Cleaning low confidence data points\n",
    "- Egocentric alignment using key reference points\n",
    "- Outlier cleaning\n",
    "- Savitzky-Golay filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d8eb6-65ff-4a6c-af5f-b0100e4edaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vame.preprocessing(\n",
    "    config=config_data,\n",
    "    centered_reference_keypoint=\"snout\",\n",
    "    orientation_reference_keypoint=\"tailbase\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc98082-c46f-4afe-b544-f84328ee3eb0",
   "metadata": {},
   "source": [
    "## Step 3: Train the VAME model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0eba3-ee31-46d5-9d6a-6d0c404443ad",
   "metadata": {},
   "source": [
    "We start by splitting the data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9a8d6-2304-4c3d-b952-834f504885c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vame.create_trainset(config=config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c860b1d-f3e8-4526-b243-0ef11631a66e",
   "metadata": {},
   "source": [
    "Now we can train the VAME model. This migth take a while, depending on dataset size and your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dca7b-383c-4a20-b205-89ed6bc9c991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vame.train_model(config=config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c5afb-ba30-491a-a791-d9bac8e65092",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the VAME model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22cf723-7c2f-4531-bd25-da454df626d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vame.evaluate_model(config=config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08837aa0-3b43-43fb-8463-323f731badfb",
   "metadata": {},
   "source": [
    "## Step 5: Segmenting the behavior into motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276c865-bf49-4e30-8a61-facf35e59a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vame.segment_session(config=config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33133e-dd21-4236-8e59-6d614356c211",
   "metadata": {},
   "source": [
    "## Step 6: Create behavioural hierarchies via community clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad952e-01d8-4c2d-8564-afa5366d4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "vame.community(\n",
    "    config=config_data,\n",
    "    segmentation_algorithm='hmm',\n",
    "    cut_tree=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08230379-d447-4f35-a2bc-a6b0b05b7bb8",
   "metadata": {},
   "source": [
    "## Step 7: Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5526d31-bfe1-450d-9544-110f2c75810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vame.visualization.motif import visualize_motif_tree\n",
    "from vame.visualization.umap import visualize_umap\n",
    "from vame.visualization.preprocessing import (\n",
    "    visualize_preprocessing_scatter,\n",
    "    visualize_preprocessing_timeseries,\n",
    ")\n",
    "from vame.visualization.model import plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19322883-8e67-4b6b-b485-0d75a1262316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_preprocessing_scatter(config=config_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b72ac-c1a6-4550-8dff-c9e13d49f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_preprocessing_timeseries(config=config_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b45c89-1622-4e8d-916a-d8f4d8e580fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(cfg=config_data, model_name=\"VAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178225b7-6c70-4a20-a9f0-67ba4cf73b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_motif_tree(\n",
    "    config=config_data,\n",
    "    segmentation_algorithm=\"hmm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8762e2e-7e84-40dc-b302-7e5314f3f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_umap(\n",
    "    config=config_data,\n",
    "    label=\"community\",\n",
    "    segmentation_algorithm=\"hmm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393bd15-9b1f-440b-a037-6c0d37fa5574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea91412d",
   "metadata": {},
   "source": [
    "---\n",
    "#### The following are optional choices to create motif videos, communities/hierarchies of behavior and community videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Create motif videos to get insights about the fine grained poses\n",
    "vame.motif_videos(config, video_type='.mp4', segmentation_algorithm='hmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3425c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Create community videos to get insights about behavior on a hierarchical scale\n",
    "vame.community_videos(config, segmentation_algorithm='hmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPTIONAL: Use the generative model (reconstruction decoder) to sample from\n",
    "# # the learned data distribution, reconstruct random real samples or visualize\n",
    "# # the cluster center for validation\n",
    "vame.generative_model(config, mode=\"sampling\", segmentation_algorithm='hmm') #options: mode: \"sampling\", \"reconstruction\", \"centers\", \"motifs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Create a video of an egocentrically aligned mouse + path through\n",
    "# the community space (similar to our gif on github) to learn more about your representation\n",
    "# and have something cool to show around ;)\n",
    "# Note: This function is currently very slow. Once the frames are saved you can create a video\n",
    "# or gif via e.g. ImageJ or other tools\n",
    "vame.gif(config, segmentation_algorithm='hmm', pose_ref_index=[0,5], subtract_background=True, start=None,\n",
    "         length=30, max_lag=30, label='community', file_format='.mp4', crop_size=(300,300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe98abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vame] *",
   "language": "python",
   "name": "conda-env-vame-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
