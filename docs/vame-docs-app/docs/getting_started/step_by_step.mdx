---
title: VAME step-by-step
sidebar_position: 2
---

The VAME workflow consists of four main steps, plus optional steps to analyse your data:
1. Initialize project: This is step is responsible by starting the project, getting your pose estimation data into the right format
2. Preprocess: This step is responsible for cleaning, filtering and aligning the pose estimation data
3. Train neural network:
    - Create a training dataset for the VAME deep learning model.
    - Train a variational autoencoder which is parameterized with recurrent neural network to embed behavioural dynamics.
    - Evaluate the performance of the trained model based on its reconstruction capabilities
4. Segment behavior:
    - Segment pose estimation time series into behavioral motifs, using HMM or K-means.
    - Group similar motifs into communities, using hierarchical clustering.
5. Analysis:
    - Optional: Create motif videos to get insights about the fine grained poses.
    - Optional: Create community videos to get more insights about behaviour on a hierarchical scale.
    - Optional: Visualization and projection of latent vectors onto a 2D plane via UMAP.
    - Optional: Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize the cluster centre for validation.
    - Optional: Create a video of an egocentrically aligned animal + path through the community space.


In this tutorial we will show you how to run the VAME workflow using a simple example. The code below can also be found in the `demo.py` script in our Github repository ([link here](https://github.com/EthoML/VAME/blob/main/examples/demo.py)).

:::tip
Check out also the published VAME Workflow Guide, including more hands-on recommendations [HERE](https://www.nature.com/articles/s42003-022-04080-7#Sec8).
:::

:::tip
You can run an entire VAME workflow with just a few lines, using the [Pipeline method](/docs/getting_started/pipeline).
:::


## 0. [Optional] Download input data
To run VAME you will need a video and a pose estimation file. If you don't have your own data, you download sample data:

```python
from vame.util.sample_data import download_sample_data

source_software = "DeepLabCut"              # "DeepLabCut", "SLEAP" or "LightningPose"
ps = download_sample_data(source_software)  # Data will be downloaded to ~/.movement/data/
videos = [ps["video"]]                      # List of paths to the video files
poses_estimations = [ps["poses"]]           # List of paths to the pose estimation files
```


## 1. Initialize the project
VAME organizes around projects. To start a new project, you need to define a few things:

```python
import vame

working_directory = '.'      # The directory where the project will be saved
project = 'my-vame-project'  # The name of the project

# [Optional] Customized configuration for the project
config_kwargs = {
    "n_clusters": 15,
    "pose_confidence": 0.9,
    "max_epochs": 100,
}

config_path, config = vame.init_new_project(
    project_name=project_name,
    videos=videos,
    poses_estimations=poses_estimations,
    source_software=source_software,
    working_directory=working_directory,
    config_kwargs=config_kwargs,
)
```

This command will create a project folder in the defined working directory with the project name you defined.
In this folder you can find a config file called `config.yaml` which holds the main parameters for the VAME workflow.
The videos and pose estimation files will be linked or copied to the project folder.


#### 2.3 Egocentric alignment
If your data is not egocentrically aligned, you can align it by running the following code:

```python
vame.egocentric_alignment(config, pose_ref_index=[0, 5])
```

But if your experiment is by design egocentrical (e.g. head-fixed experiment on treadmill etc) you can use the following to convert your .csv to a .npy array, ready to train vame on it.

```python
vame.csv_to_numpy(config)
```

#### 2.4 Creating the training dataset

To create the training dataset you can run the following code:

```python
vame.create_trainset(config, pose_ref_index=[0,5])
```


#### 2.5 Training the model
Training the vame model might take a while depending on the size of your dataset and your machine settings. To train the model you can run the following code:
```python
vame.train_model(config)
```

#### 2.6 Evaluate the model
THe model evaluation produces two plots, one showing the loss of the model during training and the other showing the reconstruction and future prediction of input sequence.
```python
vame.evaluate_model(config)
```

#### 2.7 Segmenting the behavior
To perform pose segmentation you can run the following code:
```python
vame.pose_segmentation(config)
```


### 3. Running Optional Steps of the Pipeline
:::tip
The following steps are optional and can be run if you want to create motif VideoColorSpace, communities/hierarchies of behavior and community VideoColorSpace.
:::

#### 3.1 Creating motif videos
To create motif videos and get insights about the fine grained poses you can run:
```python
vame.motif_videos(config, videoType='.mp4')
```

#### 3.2 Run community detection
To create behavioral hierarchies and communities detection run:
```python
vame.community(config, parametrization='hmm', cut_tree=2, cohort=False)
```
It will produce a tree plot of the behavioural hierarchies using hmm motifs.

#### 3.3 Community Videos
Create community videos to get insights about behavior on a hierarchical scale.
```python
vame.community_videos(config)
```

#### 3.4 UMAP Visualization
 Down projection of latent vectors and visualization via UMAP.
```python
fig = vame.visualization(config, label=None) #options: label: None, "motif", "community"
```


#### 3.5 Generative Model (Reconstruction decoder)
Use the generative model (reconstruction decoder) to sample from the learned data distribution, reconstruct random real samples or visualize
the cluster center for validation.
```python
vame.generative_model(config, mode="centers") #options: mode: "sampling", "reconstruction", "centers", "motifs"
```

#### 3.6 Create output video
Create a video of an egocentrically aligned mouse + path through
the community space (similar to our gif on github) to learn more about your representation
and have something cool to show around.

:::warning
This function is currently very slow.
:::

```python
vame.gif(config, pose_ref_index=[0,5], subtract_background=True, start=None,
         length=500, max_lag=30, label='community', file_format='.mp4', crop_size=(300,300))
```
:::tip
Once the frames are saved you can create a video or gif via e.g. ImageJ or other tools
:::
